<!doctype html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <link rel="icon" type="image/x-icon" href="https://static.thinkingandcomputing.com/favicon.ico" />
        <title>Training neural networks: back-propagation vs. genetic algorithms | Thinking and Computing</title>
        <!-- Material Library -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/material-design-lite/1.1.3/material.min.css" integrity="sha256-8/FvSMufaMDFDGPISKeLSup0kzLXG2IjRnmpPtcyxjY=" crossorigin="anonymous">
        <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
        <!-- Fonts -->
        <link href="https://fonts.googleapis.com/css?family=Courgette|Raleway:400,200,700" rel="stylesheet" type="text/css">

        <script src="https://cdn.jsdelivr.net/jquery/3.0.0-beta1/jquery.min.js"></script>

        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="tnc-background"></div>
        <script>
(function() {
    var imageNames = [
        
        './images/cover/bridge.jpeg',
        
        './images/cover/river.jpg',
        
        './images/cover/sea.jpg',
        
        './images/cover/windmill.jpg',
        
    ];
    var img = imageNames[Math.floor(Math.random() * imageNames.length)];
    document.getElementById('tnc-background').style = 'background-image: url(\'/' + img + '\');';
})();
</script>

        <div id="tnc-page" class="tnc-main tnc-layout mdl-layout mdl-js-layout">
          <header class="mdl-layout__header mdl-layout__header--transparent mdl-layout__header--scroll">
            <!-- Top row, always visible -->
            <div class="mdl-layout__header-row">
              <!-- Title -->
              <span class="mdl-layout-title"><a href="../">Thinking and Computing</a></span>
              <div class="mdl-layout-spacer"></div>
              <!-- Navigation -->
              <nav class="mdl-navigation">
                  <a class="mdl-navigation__link" href="../">Home</a>
                  <a class="mdl-navigation__link" href="../about.html">About</a>
                  <a class="mdl-navigation__link" href="../pages/past-projects.html">Past Projects</a>
                  <a class="mdl-navigation__link" href="../archive.html">Archive</a>
              </nav>
            </div>
          </header>
          <div class="mdl-layout__drawer" style="z-index: 200000;">
            <span class="mdl-layout-title">Browse T&amp;C</span>
            <nav class="mdl-navigation">
              <a class="mdl-navigation__link" href="../">Home</a>
              <a class="mdl-navigation__link" href="../about.html">About</a>
              <a class="mdl-navigation__link" href="../pages/past-projects.html">Past Projects</a>
              <a class="mdl-navigation__link" href="../archive.html">Archive</a>
            </nav>
          </div>
          <main class="tnc-main mdl-layout__content">
            <div class="tnc-container mdl-grid">
              <div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
              <div class="tnc-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
                  <h1>Training neural networks: back-propagation vs. genetic algorithms</h1>
<div class="info">
    Posted on March  9, 2014
    
</div>

<p>One of the most endemic problem of using back propagation in training artificial neural networks is that the algorithm does not guarantee finding a global minimum i.e. a network whose weights produce the lowest classification error among all possible combinations and values of weights. This is because back propagation treats learning as an optimization problem, and uses <a href="http://en.wikipedia.org/wiki/Gradient_descent" title="Gradient descent">gradient descent</a> to (hopefully) approximate the best solution. But gradient descent has inherent limitations that prevent this from happening. </p>
<div class="figure">
<img src="https://static.thinkingandcomputing.com/2014/03/bprop.png" alt="Gradient descent stuck at local minima" />
<p class="caption">Gradient descent stuck at local minima</p>
</div>
<p>Referring to the figure above, if the starting point for gradient descent was chosen inappropriately, more iterations of the algorithm will only make it approach a local minimum, never reaching the global one.</p>
<p>Therefore, back propagation is only a local optimization algorithm. To genuinely find the best neural network, one would have to use a global optimization algorithm, one that has the potential to traverse the entire search space, while remaining time-efficient.</p>
<p>One of the algorithms vaunted for this property is genetic algorithm (GA). It attempts to apply the principles of natural selection on a population of candidates, performing sporadic random mutation, crossing over values from stronger parents to produce child generations and eliminating weak candidates. In the case of neural networks, the output error can act as a measure of candidate strength. The assumption is that if two parents are “strong”, or producing low error, the child generated using a mixture of traits from each parent should also be strong, perhaps even stronger. This allows GA to fine-tune its search space, focusing only on regions likely to have minima. </p>
<div class="figure">
<img src="https://static.thinkingandcomputing.com/2014/03/crossover.png" alt="Example of crossover operation in genetic algorithm" />
<p class="caption">Example of crossover operation in genetic algorithm</p>
</div>
<p>In the picture above, each bit in the child text is taken randomly from a parent, resulting in a mixture of features thus laying out new directions for search based on intuition. </p>
<p>However, GA is not the panacea when it comes to mathematical optimization. The assumption of children being roughly as efficient as their parents does not always hold. Consider the proof-of-work used in the Bitcoin protocol: finding an input to the SHA-256 function for which the output is lower than a specified target. Genetic optimization cannot be applied to accelerate the process because of the avalanche effect. The SHA-265 function is simply too nonlinear for GA to optimize. Two parent inputs whose corresponding outputs are close to the target, when crossed, will more often than not result in children with seemingly random outputs.</p>
<p>Similarly, a formidable problem surfaces when GA is used to train neural networks. Due to their unique structure, neural networks may not retain their performance when undergone the cross-over operation. Two networks may have different internal structures, but still give identical outputs. See illustration below: </p>
<div class="figure">
<img src="https://static.thinkingandcomputing.com/2014/03/nn.png" alt="Two neural networks" />
<p class="caption">Two neural networks</p>
</div>
<p>The two networks are essentially mirror reflections of each other, hence produce the same output. But if the cross-over operation is applied, the behavior of the resulting neural network will deviate significantly from that of the parents. Since GA is unaware of the internal structures of the two networks, it will combine weights belonging to nodes of different roles. Hence two superior networks of different structure, when crossed, may result in offspring performing poorly.</p>
<p>The crux of the problem lies in the encoding method for neural networks. Network weights are generally stored based on a fixed indexing scheme: nodes are numbered sequentially, laterally regardless of their significance and impact on the next layer. This causes GA to confuse nodes of different roles. Until an intelligent approach to determine nodes of similar roles is devised and the corresponding role-based encoding method used, the efficacy of GA in training neural networks will be much limited. The method must give the same encoded result for both network A and B above, since they are, after all, functionally identical networks.</p>

<div class="row-fluid">
    <h2>Disqus Comments</h2>
    <div id="disqus_thread"></div>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<script>
(function(jq){
    var ds_loaded = false, top = jq("#disqus_thread").offset().top;
    window.disqus_developer = 1;
    window.disqus_shortname = 'thinkingandcomputing';

    var page = jq("#tnc-page");
    var wind = jq(window);
    function check() {
        if ( !ds_loaded && page.scrollTop() + wind.height() > top ) {
            jq.ajax({
                type: "GET",
                url: "https://" + disqus_shortname + ".disqus.com/embed.js",
                dataType: "script",
                cache: true
            });
            ds_loaded = true;
        }
    }

    page.scroll(check);

    check();
})(jQuery);
</script>

              </div>
            </div>
          </main>
        </div>

        <script src="https://cdn.jsdelivr.net/material-design-lite/1.1.3/material.min.js" integrity="sha256-qeJNkhp5/Tnaa3Ovx49//j+Kn0Lx9ykNYJdLMxCwd1c=" crossorigin="anonymous" defer async></script>
    </body>
</html>
