<!doctype html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>HHVM plus Nginx on DigitalOcean: a comprehensive guide | Thinking and Computing</title>
        <!-- Material Library -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/material-design-lite/1.1.3/material.min.css" integrity="sha256-8/FvSMufaMDFDGPISKeLSup0kzLXG2IjRnmpPtcyxjY=" crossorigin="anonymous">
        <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
        <!-- Fonts -->
        <link href="https://fonts.googleapis.com/css?family=Courgette|Raleway:400,200,700" rel="stylesheet" type="text/css">

        <script src="https://cdn.jsdelivr.net/jquery/3.0.0-beta1/jquery.min.js"></script>

        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="tnc-background"></div>
        <script>
(function() {
    var imageNames = [
        
        'images/cover/jump.jpg',
        
        'images/cover/newyork.jpg',
        
        'images/cover/river.jpg',
        
        'images/cover/windmill.jpg',
        
    ];
    var img = imageNames[Math.floor(Math.random() * imageNames.length)];
    document.getElementById('tnc-background').style = 'background-image: url(\'/' + img + '\');';
})();
</script>

        <div class="tnc-main tnc-layout mdl-layout mdl-js-layout">
          <header class="mdl-layout__header mdl-layout__header--transparent mdl-layout__header--scroll">
            <!-- Top row, always visible -->
            <div class="mdl-layout__header-row">
              <!-- Title -->
              <span class="mdl-layout-title"><a href="../">Thinking and Computing</a></span>
              <div class="mdl-layout-spacer"></div>
              <!-- Navigation -->
              <nav class="mdl-navigation">
                  <a class="mdl-navigation__link" href="../">Home</a>
                  <a class="mdl-navigation__link" href="../about.html">About</a>
                  <a class="mdl-navigation__link" href="../pages/past-projects.html">Past Projects</a>
                  <a class="mdl-navigation__link" href="../archive.html">Archive</a>
              </nav>
            </div>
          </header>
          <div class="mdl-layout__drawer">
            <span class="mdl-layout-title">Browse T&amp;C</span>
            <nav class="mdl-navigation">
              <a class="mdl-navigation__link" href="../">Home</a>
              <a class="mdl-navigation__link" href="../about.html">About</a>
              <a class="mdl-navigation__link" href="../pages/past-projects.html">Past Projects</a>
              <a class="mdl-navigation__link" href="../archive.html">Archive</a>
            </nav>
          </div>
          <main class="tnc-main mdl-layout__content">
            <div class="tnc-container mdl-grid">
              <div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
              <div class="tnc-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
                  <h1>HHVM plus Nginx on DigitalOcean: a comprehensive guide</h1>
<div class="info">
    Posted on October 23, 2014
    
</div>

<p>After dithering between multiple PaaS options, I finally decided to settle down on a dedicated VPS for my blog. The motivation behind is three-fold: I get to enjoy fully static IP address which enables naked domain resolution (ditching WWW!); modifications to the local file system are preserved, a sine qua non for powerful caching plugins such as W3 Total Cache; last but not least, with SSDs and state-of-the-art Xeon processors, the relative performance is higher. </p>
<p>Seamlessly migrating a Wordpress blog is not particularly straightforward. Platform differences between hosting partners, compounded by a plethora of plugins-cum-configurations necessitated nearly a day for thorough setup. In this post however, I will strip out the nitty-gritty details of blog migration and cover the essentials in setting up HHVM+Nginx on a DigitalOcean VPS.</p>
<h2 id="setting-up-a-droplet">Setting up a droplet</h2>
<p>If one were to consider “Ocean” as to comprising countless “droplets”, the eponymous server building block is ostensibly a deliberate play on the company’s title i.e. DitigalOcean. The virtual server has many Linux flavors. I chose Ubuntu 14.04 as it appeared to have the most recent software packages.</p>
<p><a href="https://static.thinkingandcomputing.com/2014/10/droplet.png"><img src="https://static.thinkingandcomputing.com/2014/10/droplet.png" alt="Droplet configuration" /></a></p>
<p>Droplet configuration</p>
<p>Be sure to check the “Private Networking” option, as it can be very helpful when expanding to more than one servers.</p>
<p><em>A word on server sizes:</em> at the time of writing, DigitalOcean does not support downsizing hard disk capacity, albeit it being a feature highly voted by the community. While this may seem like a trivial issue, the trouble comes when memory and CPU levels are bundled together with hard disk. Not being able to reduce disk quota implies to a large extent that you cannot downgrade the server at all. So do not over-provision your servers. After setting up a virtual server, you may SSH into it either from the password sent via e-mail or utilizing pre-uploaded public keys.</p>
<p>It is pertinent, before diving into configuring myriad of web services, to bolster the security of your virtual server. Two crucial measures apart from using strong passwords are apt firewall settings and depriving HHVM and Nginx processes of root privileges.</p>
<h2 id="iptables">IPTables</h2>
<p>As its name suggests, IPTables is a Linux firewall that provides the first line of defense against external threats, by matching connections against a list of defined rules. Prior to altering the IPTables policies, we can use this command to see if there are any, which should not be present in the case of a stock Ubuntu intallation:</p>
<pre><code>iptables -L -n</code></pre>
<p>The following lines set the default action to accept for connections not matching any rules, and do a flush to obliterate any existing rules. It is crucial that the default actions are set to ACCEPT before the rules are flushed, or one may be locked out of the machine. The default ACCEPT policies are of course only transitory since no one wants to expose the server to all sorts of traffic.</p>
<div class="sourceCode" startFrom="1"><table class="sourceCode shell numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
</pre></td><td class="sourceCode"><pre><code class="sourceCode">sudo iptables -P INPUT ACCEPT
sudo iptables -P OUTPUT ACCEPT
sudo iptables -P FORWARD ACCEPT
sudo iptables -F</code></pre></td></tr></table></div>
<p>The only vital incoming connections to the public network are SSH (to log in) and HTTP (to server web pages), thus we will solely enable them:</p>
<pre><code>sudo iptables -A INPUT -p tcp --dport 22 -i eth0 -j ACCEPT
sudo iptables -A INPUT -p tcp --dport 80 -i eth0 -j ACCEPT</code></pre>
<p>Here eth0 refers to the public network interface on DigitalOcean VPS, while 22 and 80 represents the port number of SSH and HTTP respectively.</p>
<p>Next we permit loopback connections and public connections initiated by the server.</p>
<pre><code>sudo iptables -A INPUT -i lo -j ACCEPT
sudo iptables -A OUTPUT -o lo -j ACCEPT
sudo iptables -I OUTPUT -o eth0 -j ACCEPT</code></pre>
<p>If you are hosting your database at another server, there should also be rules explicitly allowing connections to that server in the private network. Interface eth1 is the private ethernet interface in DigitalOcean.</p>
<pre><code>sudo iptables -A OUTPUT -o eth1 -p tcp -d &lt;db_server_ip&gt; --dport 3306 -j ACCEPT
sudo iptables -A INPUT -i eth1 -p tcp -j ACCEPT</code></pre>
<p>When finished with configuration, set default policy for all other connections to DROP:</p>
<pre><code>iptables -P INPUT DROP
iptables -P OUTPUT DROP
iptables -P FORWARD DROP</code></pre>
<p>Caveat: IPTables rules are by default not set to survive reboots. To retain your fastidiously written rules there is a convenient package called iptables-persistent. Install it and follow the guides to save the IPTables config.</p>
<pre><code>apt-get install iptables-persistent</code></pre>
<h2 id="install-fail2ban">Install Fail2Ban</h2>
<p>Your virtual server is not a secluded island in the expansive Internet. If you are confident that you will only be accessing it from a few static IP addresses, you can whitelist those IPs in SSH service. For the vast majority, static IP may well be a luxury, and consequently the SSH service must be exposed to the Internet (unless you proxy SSH over a trusted server. This digresses into another topic). While the IP adress can be hidden from public, the Internet is replete with sharks lurking around, targeting VPS IP ranges using port scanners and brute-force cracking utilities. It is common to be greeted by your server with such distressing sentences:</p>
<pre><code>Last failed login: Sun Oct 26 21:06:46 EDT 2014 from 88.135.1.99 on ssh:notty
There were 13026 failed login attempts since the last successful login.</code></pre>
<p>Tracing the IP leads to some obscure location in Russia and <a href="http://www.abuseipdb.com/report-history/88.135.1.99">a litany of offenses</a>.</p>
<p>A stopgap measure is to install Fail2Ban, a program that monitors rejected login attempts and bans them temporarily using the aforementioned IPTable.</p>
<pre><code>apt-get install fail2ban</code></pre>
<p>The default configuration blocks any address temporarily for 10 minutes after 3 unsuccessful logins.</p>
<h2 id="relegating-hhvm-and-nginx-to-an-unprivileged-user">Relegating HHVM and Nginx to an unprivileged user</h2>
<p>Considerable number of attacks uses web servers as a springboard to launch other malicious processes. By limiting access to the system we effectively curbs the degree of damage they can do. At this stage we only create a new user using the adduser command, leaving details on how to handle the permissions later.</p>
<pre><code># Creates a new user named &quot;webuser&quot;.
# You will be prompted to enter password and sundry information.
adduser webuser</code></pre>
<p>Now, with certain level of security in check, we may move on to installing software packages. First up, HHVM.</p>
<h2 id="hhvm-on-ubuntu">HHVM on Ubuntu</h2>
<p>HHVM binaries are actively maintained. To get them we merely have to import the HHVM repository and install it from there:</p>
<pre><code>wget -O - http://dl.hhvm.com/conf/hhvm.gpg.key | sudo apt-key add -
echo deb http://dl.hhvm.com/ubuntu trusty main | sudo tee /etc/apt/sources.list.d/
echo deb http://dl.hhvm.com/ubuntu trusty main | sudo tee /etc/apt/sources.list.d/hhvm.list

sudo apt-get update
sudo apt-get install hhvm
hhvm --version</code></pre>
<p>If everything functions as intended, the last command should output the HHVM version information, which for me was 3.3.0.</p>
<h2 id="installingnginx-on-ubuntu">Installing Nginx on Ubuntu</h2>
<p>The stock Nginx version is a bit obsolete; 1.4.7 was an old branch with latest security fixes, but fewer features. The Nginx team maintains a repository with latest versions. Adding it and installing Nginx is simple:</p>
<pre><code>sudo add-apt-repository ppa:nginx/development
sudo apt-get install nginx</code></pre>
<h2 id="compiling-nginx-on-ubuntu-deprecated">Compiling Nginx on Ubuntu (Deprecated)</h2>
<p><em>This section describes how to compile Nginx on your own, which is not the recommended way of installing Nginx but may provide more flexibility.</em></p>
<p>To obtain the most recent source and compile it, execute the following commands, substituting 1.7.6 with whichever version desired.</p>
<pre><code>apt-get install libpcre3-dev build-essential libssl-dev dpkg-dev build-essential zlib1g-dev libpcre3 libpcre3-dev unzip libxslt-dev libgeoip-dev geoip-database
NGINX_VERSION=1.7.6
wget http://nginx.org/download/nginx-${NGINX_VERSION}.tar.gz
tar -xvzf nginx-${NGINX_VERSION}.tar.gz
cd nginx-${NGINX_VERSION}/

./configure \
--with-http_ssl_module \
--with-pcre-jit \
--with-ipv6 \
--with-http_ssl_module \
--with-http_stub_status_module \
--with-http_realip_module \
--with-http_addition_module \
--with-http_geoip_module \
--with-http_gzip_static_module \
--with-http_spdy_module \
--with-http_sub_module \
--with-http_xslt_module \
--with-mail \
--with-mail_ssl_module \
--with-file-aio

CFLAGS=&quot;-O3&quot; CXXFLAGS=&quot;-O3&quot; make
make install
cp objs/nginx /usr/local/sbin/</code></pre>
<p>Some lines worth mentioning: I had to manually specify the optimization levels when invoking make, otherwise the compiler will use the default optimization level, resulting in an exceedingly large binary with abysmal performance. Furthermore, you can tune the list of modules compiled into the final binary, though the provided bunch should suffice under most daily usage.</p>
<h2 id="setting-up-supervisord">Setting up Supervisord</h2>
<p>At the moment, although circumstances have been improving, HHVM is prone to sporadic crashes. To minimize their impact on server uptime, I used a monitoring daemon called Supervisord. It spawns stipulated executables at start up and attempts to restart them should any process exit unexpectedly. To install Superviosrd, a program named easy_install is required, which belongs to the python-setuptools package:</p>
<pre><code>apt-get install python-setuptools
easy_install supervisor</code></pre>
<p>The default config file for Supervisord is located at <code>/etc/supervisord.conf.</code> First we may turn off the built in administration server by commenting out [unix_http_server] and [inet_http_server] headers, following which are the settings for each process to be managed. Below is a sample configuration for HHVM. Nginx, on the other hand, is relatively much less susceptible to crashes and can be installed as a service.</p>
<pre><code>[program:hhvm]
command=hhvm -m server -vServer.Type=fastcgi
    -vServer.FileSocket=/usr/local/nginx/hhvm.sock
    -vEval.EnableZendCompat=true
    -vRepo.Central.Path=/usr/local/nginx/.hhvm.hhbc
    -vLog.UseLogFile=true -vLog.File=/home/webusr/hhvm.log
    -c /etc/hhvm/php.ini
directory=/usr/local/nginx/html
autostart=true            ; start at supervisord start (default: true)
user=webuser               ; setuid to this UNIX account to run the program
redirect_stderr=true      ; redirect proc stderr to stdout (default false)
stdout_logfile=~/hhvm_stdout.log   ; stdout log path, NONE for none;</code></pre>
<p>The line “user=webuser” instructs Supervisord to execute HHVM from the unprivileged account.</p>
<p>Lastly, before firing up Nginx, we need to edit its configuration file, found at /usr/local/nginx/conf/nginx.conf, to let it run the worker processes under webuser as well. Inserting this at the beginning:</p>
<p><code>user webuser;</code></p>
<p>If you intend to run sites that modifies the local file system e.g. installing plugins in Wordpress, the files and directories involved need to be “chown”ed to webuser. The command to process the entire Nginx web root is provided below:</p>
<p><code>chown -R webuser: /usr/local/nginx/html</code></p>

              </div>
            </div>
          </main>
        </div>

        <script src="https://cdn.jsdelivr.net/material-design-lite/1.1.3/material.min.js" integrity="sha256-qeJNkhp5/Tnaa3Ovx49//j+Kn0Lx9ykNYJdLMxCwd1c=" crossorigin="anonymous" defer async></script>
    </body>
</html>
