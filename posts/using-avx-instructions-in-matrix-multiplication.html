<!doctype html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <link rel="icon" type="image/x-icon" href="https://static.thinkingandcomputing.com/favicon.ico" />
        <title>Using AVX instructions in matrix multiplication | Thinking and Computing</title>
        <!-- Material Library -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/material-design-lite/1.1.3/material.min.css" integrity="sha256-8/FvSMufaMDFDGPISKeLSup0kzLXG2IjRnmpPtcyxjY=" crossorigin="anonymous">
        <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
        <!-- Fonts -->
        <link href="https://fonts.googleapis.com/css?family=Courgette|Raleway:400,200,700" rel="stylesheet" type="text/css">

        <script src="https://cdn.jsdelivr.net/jquery/3.0.0-beta1/jquery.min.js"></script>

        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="tnc-background"></div>
        <script>
(function() {
    var imageNames = [
        
        './images/cover/jump.jpg',
        
        './images/cover/newyork.jpg',
        
        './images/cover/river.jpg',
        
        './images/cover/windmill.jpg',
        
    ];
    var img = imageNames[Math.floor(Math.random() * imageNames.length)];
    document.getElementById('tnc-background').style = 'background-image: url(\'/' + img + '\');';
})();
</script>

        <div id="tnc-page" class="tnc-main tnc-layout mdl-layout mdl-js-layout">
          <header class="mdl-layout__header mdl-layout__header--transparent mdl-layout__header--scroll">
            <!-- Top row, always visible -->
            <div class="mdl-layout__header-row">
              <!-- Title -->
              <span class="mdl-layout-title"><a href="../">Thinking and Computing</a></span>
              <div class="mdl-layout-spacer"></div>
              <!-- Navigation -->
              <nav class="mdl-navigation">
                  <a class="mdl-navigation__link" href="../">Home</a>
                  <a class="mdl-navigation__link" href="../about.html">About</a>
                  <a class="mdl-navigation__link" href="../pages/past-projects.html">Past Projects</a>
                  <a class="mdl-navigation__link" href="../archive.html">Archive</a>
              </nav>
            </div>
          </header>
          <div class="mdl-layout__drawer" style="z-index: 200000;">
            <span class="mdl-layout-title">Browse T&amp;C</span>
            <nav class="mdl-navigation">
              <a class="mdl-navigation__link" href="../">Home</a>
              <a class="mdl-navigation__link" href="../about.html">About</a>
              <a class="mdl-navigation__link" href="../pages/past-projects.html">Past Projects</a>
              <a class="mdl-navigation__link" href="../archive.html">Archive</a>
            </nav>
          </div>
          <main class="tnc-main mdl-layout__content">
            <div class="tnc-container mdl-grid">
              <div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
              <div class="tnc-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
                  <h1>Using AVX instructions in matrix multiplication</h1>
<div class="info">
    Posted on February 28, 2014
    
</div>

<p>Recent Intel processors such as SandyBridge and IvyBridge have incorporated an instruction set called Advanced Vector Extensions, or AVX. This new addition to the spectrum of SIMD instructions makes the CPU even faster at crunching large amounts of floating point data. Matrix multiplication is a great candidate for performing optimizations via SIMD, since it involves mutually-independent multiplication and summing. To take advantage of the speed up, one could certainly inline a couple of assembly instructions. But this method is both inelegant and non-portable. The preferred approach is to use intrinsics instead. </p>
<p>What are instrinsics? Loosely speaking, intrinsics are functions that provide functionality equivalent to a few lines of assembly. Hence the assembly version can serve as a drop-in replacement to the function at compile-time. This allows performance-critical assembly code to be inlined without sacrificing the beauty and readability of the high-level language syntax. For example, the intrinsic function <code>InterlockedExchange(LONG volatile *target, LONG value)</code> atomically sets the data at the memory address held by target to value. This function will be replaced by a single inlined assembly <code>xchg [target], value</code> during compile time. </p>
<p>The <a href="http://software.intel.com/sites/landingpage/IntrinsicsGuide/" title="Intel Intrinsics Guide">Intel Intrinsics Guide</a> provides detailed reference on SIMD instructions and their respective intrinsic versions. However, the GCC naming conventions for intrinsic functions are different from those used in Intel, hence additional lookups are required. </p>
<p>There are three types of instructions used:  load: moving data from memory to registers for calculation arithmetic: batch operation on floating-point numbers in registers and finally, store: moving data from registers back to memory for future processing</p>
<p>The respective instructions are listed below:</p>
<table>
<colgroup>
<col width="16%" />
<col width="58%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Assembly</th>
<th align="left">Description</th>
<th align="left">GCC intrinsics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">vmovups dst, addr</td>
<td align="left">Load 256-bits (8 float elements) from memory into dst.</td>
<td align="left"><code>__builtin_ia32_loadups256</code></td>
</tr>
<tr class="even">
<td align="left">vmulps dst, a, b</td>
<td align="left">Multiply float elements in a and b, and store the results in dst.</td>
<td align="left"><code>__builtin_ia32_mulps256</code></td>
</tr>
<tr class="odd">
<td align="left">vaddps dst, a, b</td>
<td align="left">Add float elements in a and b, and store the results in dst.</td>
<td align="left"><code>__builtin_ia32_addps256</code></td>
</tr>
<tr class="even">
<td align="left">vmovups addr, a</td>
<td align="left">Store 256-bits (8 float elements) from a into memory.</td>
<td align="left"><code>__builtin_ia32_storeups256</code></td>
</tr>
</tbody>
</table>
<p>Using the instrinsic versions, one could write vectorized code with ease. The following is a simple example of multiplying two groups of 8 floats: </p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="ot">#include &lt;iostream&gt;</span>
<span class="dt">extern</span> <span class="st">&quot;C&quot;</span> {
<span class="ot">#include &lt;immintrin.h&gt;</span>
}

<span class="dt">int</span> main(){
	__m256 ymm0, ymm1;			<span class="co">//define the registers used</span>
	<span class="dt">float</span> a[<span class="dv">8</span>]={<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>};
	<span class="dt">float</span> b[<span class="dv">8</span>]={<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>};
	<span class="dt">float</span> c[<span class="dv">8</span>];
	ymm0 = <span class="ot">__builtin_ia32_loadups256</span>(a);	<span class="co">//load the 8 floats in a into ymm0</span>
	ymm1 = <span class="ot">__builtin_ia32_loadups256</span>(b);	<span class="co">//load the 8 floats in b into ymm1</span>

	<span class="co">//multiply ymm0 and ymm1, store the result in ymm0</span>
	ymm0 = <span class="ot">__builtin_ia32_mulps256</span>(ymm0, ymm1);
	<span class="ot">__builtin_ia32_storeups256</span>(c, ymm0);	<span class="co">//copy the 8 floats in ymm0 to c</span>

	<span class="kw">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i&lt;<span class="dv">8</span>; i++)
		std::cout&lt;&lt;c[i]&lt;&lt;<span class="st">&quot;, &quot;</span>;
	<span class="kw">return</span> <span class="dv">0</span>;
}</code></pre></div>
<p>Note that the header <code>&lt;immintrin.h&gt;</code> must be included.</p>
<p>Compile with:</p>
<pre><code>g++ -mavx main.cpp -o test</code></pre>
<p>The -mavx switch enables gcc to emit avx instructions. Without it, the program will not compile. Once run, the program should output “2, 6, 12, 20, 30, 42, 56, 72,”. </p>
<p>To keep it simple, I first started with a reduced version of matrix multiplication: multiplying an n x m matrix with an m x 1 matrix, which is essentially a vector. E.g.</p>
<div class="figure">
<img src="https://static.thinkingandcomputing.com/2014/02/CodeCogsEqn.png" alt="Matrix multiplication equation" />
<p class="caption">Matrix multiplication equation</p>
</div>
<p><span>This is achieved by setting the element of the last matrix at the i</span><sup>th</sup> <span style="line-height: 1.714285714; font-size: 1rem;">row and j</span><sup>th</sup> <span>column to be the dot product between i</span><sup>th</sup> <span>row of the first matrix and j</span><sup>th</sup> <span>column of the second matrix. </span></p>
<p>To do the same process efficiently using AVX code, one could load up to 8 groups of 8 floats per matrix in an iteration, using the <code>__builtin_ia32_loadups256</code> function, after which respective float numbers in the two matrix would be multiplied, and the results summed to give the dot product. The advantage of this approach is that there is prospect of further instruction-level parallelism and also less overhead in loop.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="ot">#include &lt;time.h&gt;</span>
<span class="dt">extern</span> <span class="st">&quot;C&quot;</span>
{
<span class="ot">#include &lt;immintrin.h&gt;</span>
}

<span class="kw">using</span> <span class="kw">namespace</span> std;

<span class="dt">int</span> main() {
  <span class="dt">const</span> <span class="dt">int</span> col = <span class="dv">128</span>, row = <span class="dv">24</span>, num_trails = <span class="dv">10000000</span>;

  <span class="dt">float</span> w[row][col];
  <span class="dt">float</span> x[col];
  <span class="dt">float</span> y[row];
  <span class="dt">float</span> scratchpad[<span class="dv">8</span>];
  <span class="kw">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i&lt;row; i++) {
    <span class="kw">for</span> (<span class="dt">int</span> j=<span class="dv">0</span>; j&lt;col; j++) {
      w[i][j]=(<span class="dt">float</span>)(rand()%<span class="dv">1000</span>)/<span class="fl">800.0f</span>;
    }
  }
  <span class="kw">for</span> (<span class="dt">int</span> j=<span class="dv">0</span>; j&lt;col; j++) {
    x[j]=(<span class="dt">float</span>)(rand()%<span class="dv">1000</span>)/<span class="fl">800.0f</span>;
  }

  clock_t t1, t2;

  t1 = clock();
  <span class="kw">for</span> (<span class="dt">int</span> r = <span class="dv">0</span>; r &lt; num_trails; r++)
    <span class="kw">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; row; j++)
    {
      <span class="dt">float</span> sum = <span class="dv">0</span>;
      <span class="dt">float</span> *wj = w[j];

      <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; col; i++)
        sum += wj[i] * x[i];

      y[j] = sum;
    }
  t2 = clock();
  <span class="dt">float</span> diff = (((<span class="dt">float</span>)t2 - (<span class="dt">float</span>)t1) / CLOCKS_PER_SEC ) * <span class="dv">1000</span>;
  cout&lt;&lt;<span class="st">&quot;Time taken: &quot;</span>&lt;&lt;diff&lt;&lt;endl;

  <span class="kw">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i&lt;row; i++) {
    cout&lt;&lt;y[i]&lt;&lt;<span class="st">&quot;, &quot;</span>;
  }
  cout&lt;&lt;endl;

  __m256 ymm0, ymm1, ymm2, ymm3, ymm4, ymm5, ymm6, ymm7,
    ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

  t1 = clock();
  <span class="dt">const</span> <span class="dt">int</span> col_reduced = col - col%<span class="dv">64</span>;
  <span class="dt">const</span> <span class="dt">int</span> col_reduced_32 = col - col%<span class="dv">32</span>;
  <span class="kw">for</span> (<span class="dt">int</span> r = <span class="dv">0</span>; r &lt; num_trails; r++)
    <span class="kw">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i&lt;row; i++) {
      <span class="dt">float</span> res = <span class="dv">0</span>;
      <span class="kw">for</span> (<span class="dt">int</span> j=<span class="dv">0</span>; j&lt;col_reduced; j+=<span class="dv">64</span>) {
        ymm8 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j]);
        ymm9 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+8</span>]);
        ymm10 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+16</span>]);
        ymm11 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+24</span>]);
        ymm12 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+32</span>]);
        ymm13 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+40</span>]);
        ymm14 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+48</span>]);
        ymm15 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+56</span>]);

        ymm0 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j]);
        ymm1 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+8</span>]);
        ymm2 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+16</span>]);
        ymm3 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+24</span>]);
        ymm4 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+32</span>]);
        ymm5 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+40</span>]);
        ymm6 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+48</span>]);
        ymm7 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+56</span>]);

        ymm0 = <span class="ot">__builtin_ia32_mulps256</span>(ymm0, ymm8 );
        ymm1 = <span class="ot">__builtin_ia32_mulps256</span>(ymm1, ymm9 );
        ymm2 = <span class="ot">__builtin_ia32_mulps256</span>(ymm2, ymm10);
        ymm3 = <span class="ot">__builtin_ia32_mulps256</span>(ymm3, ymm11);
        ymm4 = <span class="ot">__builtin_ia32_mulps256</span>(ymm4, ymm12);
        ymm5 = <span class="ot">__builtin_ia32_mulps256</span>(ymm5, ymm13);
        ymm6 = <span class="ot">__builtin_ia32_mulps256</span>(ymm6, ymm14);
        ymm7 = <span class="ot">__builtin_ia32_mulps256</span>(ymm7, ymm15);

        ymm0 = <span class="ot">__builtin_ia32_addps256</span>(ymm0, ymm1);
        ymm2 = <span class="ot">__builtin_ia32_addps256</span>(ymm2, ymm3);
        ymm4 = <span class="ot">__builtin_ia32_addps256</span>(ymm4, ymm5);
        ymm6 = <span class="ot">__builtin_ia32_addps256</span>(ymm6, ymm7);
        ymm0 = <span class="ot">__builtin_ia32_addps256</span>(ymm0, ymm2);
        ymm4 = <span class="ot">__builtin_ia32_addps256</span>(ymm4, ymm6);
        ymm0 = <span class="ot">__builtin_ia32_addps256</span>(ymm0, ymm4);

        <span class="ot">__builtin_ia32_storeups256</span>(scratchpad, ymm0);
        <span class="kw">for</span> (<span class="dt">int</span> k=<span class="dv">0</span>; k&lt;<span class="dv">8</span>; k++)
          res += scratchpad[k];
      }
      <span class="kw">for</span> (<span class="dt">int</span> j=col_reduced; j&lt;col_reduced_32; j+=<span class="dv">32</span>) {
        ymm8 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j]);
        ymm9 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+8</span>]);
        ymm10 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+16</span>]);
        ymm11 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;x[j<span class="dv">+24</span>]);

        ymm0 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j]);
        ymm1 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+8</span>]);
        ymm2 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+16</span>]);
        ymm3 = <span class="ot">__builtin_ia32_loadups256</span>(&amp;w[i][j<span class="dv">+24</span>]);

        ymm0 = <span class="ot">__builtin_ia32_mulps256</span>(ymm0, ymm8 );
        ymm1 = <span class="ot">__builtin_ia32_mulps256</span>(ymm1, ymm9 );
        ymm2 = <span class="ot">__builtin_ia32_mulps256</span>(ymm2, ymm10);
        ymm3 = <span class="ot">__builtin_ia32_mulps256</span>(ymm3, ymm11);

        ymm0 = <span class="ot">__builtin_ia32_addps256</span>(ymm0, ymm1);
        ymm2 = <span class="ot">__builtin_ia32_addps256</span>(ymm2, ymm3);
        ymm0 = <span class="ot">__builtin_ia32_addps256</span>(ymm0, ymm2);

        <span class="ot">__builtin_ia32_storeups256</span>(scratchpad, ymm0);
        <span class="kw">for</span> (<span class="dt">int</span> k=<span class="dv">0</span>; k&lt;<span class="dv">8</span>; k++)
          res += scratchpad[k];
      }
      <span class="kw">for</span> (<span class="dt">int</span> l=col_reduced_32; l&lt;col; l++) {
        res += w[i][l] * x[l];
      }
      y[i] = res;
    }
  t2 = clock();
  diff = (((<span class="dt">float</span>)t2 - (<span class="dt">float</span>)t1) / CLOCKS_PER_SEC ) * <span class="dv">1000</span>;
  cout&lt;&lt;<span class="st">&quot;Time taken: &quot;</span>&lt;&lt;diff&lt;&lt;endl;

  <span class="kw">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i&lt;row; i++) {
    cout&lt;&lt;y[i]&lt;&lt;<span class="st">&quot;, &quot;</span>;
  }
  cout&lt;&lt;endl;

  <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre></div>
<p>This algorithm has three versions of matrix-vector multiplication, namely the SIMD version using all YMM registers which handles column number up to the greatest multiple of 64, a similar version using half the registers handling the remaining column number up to the greatest multiple of 32, and a simple loop-based approach catering to the rest of situations.</p>
<p>Compile with:</p>
<pre><code>g++ -O3 -mavx main.cpp -o avxtest</code></pre>
<p>Running avxtest:</p>
<pre><code>$ ./avxtest
Time taken for serial version: 3980
5831.11, 6747, 6506.54, 6451.99, 5929.76, 6637.35,
Time taken for AVX version: 510
5831.11, 6747, 6506.54, 6451.99, 5929.76, 6637.35,</code></pre>
<p>As seen from the timing, using AVX resulted in nearly 8x speed up, which is expected, as each instruction is able to multiply 8 floats in a row. On the contrary, the serial multiplication algorithm, even under maximum optimization settings, was not automatically vectorized by gcc.</p>
<script type="text/javascript">jQuery(document).ready(function() {
jQuery('.collapseomatic:not(.colomat-close)').each(function(index) {
		var thisid = jQuery(this).attr('id');
		jQuery('#target-'+thisid).css('display', 'none');
	});
});</script>

<div class="row-fluid">
    <h2>Disqus Comments</h2>
    <div id="disqus_thread"></div>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<script>
(function(jq){
    var ds_loaded = false, top = jq("#disqus_thread").offset().top;
    window.disqus_developer = 1;
    window.disqus_shortname = 'thinkingandcomputing';

    var page = jq("#tnc-page");
    var wind = jq(window);
    function check() {
        if ( !ds_loaded && page.scrollTop() + wind.height() > top ) {
            jq.ajax({
                type: "GET",
                url: "https://" + disqus_shortname + ".disqus.com/embed.js",
                dataType: "script",
                cache: true
            });
            ds_loaded = true;
        }
    }

    page.scroll(check);

    check();
})(jQuery);
</script>

              </div>
            </div>
          </main>
        </div>

        <script src="https://cdn.jsdelivr.net/material-design-lite/1.1.3/material.min.js" integrity="sha256-qeJNkhp5/Tnaa3Ovx49//j+Kn0Lx9ykNYJdLMxCwd1c=" crossorigin="anonymous" defer async></script>
    </body>
</html>
